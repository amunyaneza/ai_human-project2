# ai_human-project2
# ðŸŒ¸ AI vs Human Text Detector âœ¨

A comprehensive machine learning application that detects whether text content was generated by AI or written by humans. The application supports multiple ML models including traditional algorithms (SVM, Decision Tree, AdaBoost) and deep learning models (CNN, LSTM, RNN).

##  Features

- **Multi-Model Support**: Compare results across 6 different ML models
- **Single Text Analysis**: Analyze individual text samples with confidence scores
- **Batch Processing**: Upload files (.txt, .csv) for bulk analysis
- **Model Comparison**: Side-by-side comparison of all models on the same text
- **Interactive Dashboard**: Beautiful Streamlit interface with custom styling
- **Probability Visualization**: Visual charts showing prediction confidence

##  Architecture

### Model Types
- **Traditional ML**: SVM, Decision Tree, AdaBoost (using TF-IDF features)
- **Deep Learning**: CNN, LSTM, RNN (using tokenized sequences)

### Key Components
- **Frontend**: Streamlit web application with custom CSS styling
- **Backend**: Model loading, preprocessing, and prediction pipeline
- **Models**: Pre-trained models stored in `models/` directory
- **Visualization**: Plotly charts for probability comparisons

##  Prerequisites

- Python 3.7 or higher
- pip package manager
- At least 4GB RAM (for deep learning models)
- 1GB free disk space

##  Installation

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/ai-human-detector.git
cd ai-human-detector
```

### 2. Create Virtual Environment (Recommended)

```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Create Requirements File

If `requirements.txt` doesn't exist, create it with these dependencies:

```txt
streamlit>=1.28.0
tensorflow>=2.13.0
scikit-learn>=1.3.0
pandas>=2.0.0
numpy>=1.24.0
joblib>=1.3.0
plotly>=5.15.0
streamlit-option-menu>=0.3.6
```

##  Project Structure

```
ai-human-detector/
â”œâ”€â”€ app.py                    # Main Streamlit application
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ models/                  # Model files directory
â”‚   â”œâ”€â”€ svm_model.pkl           # SVM model
â”‚   â”œâ”€â”€ decision_tree_model.pkl # Decision Tree model
â”‚   â”œâ”€â”€ adaboost_model.pkl      # AdaBoost model
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl    # TF-IDF vectorizer
â”‚   â”œâ”€â”€ tokenizer.pkl           # Keras tokenizer
â”‚   â”œâ”€â”€ CNN.pkl                 # CNN model metadata
â”‚   â”œâ”€â”€ LSTM.pkl                # LSTM model metadata
â”‚   â”œâ”€â”€ RNN.pkl                 # RNN model metadata
â”‚   â”œâ”€â”€ cnn_model.h5            # CNN weights
â”‚   â”œâ”€â”€ lstm_model.h5           # LSTM weights
â”‚   â””â”€â”€ rnn_model.h5            # RNN weights
â”œâ”€â”€ data/                    # Training data (optional)
â”œâ”€â”€ notebooks/               # Jupyter notebooks for training
â””â”€â”€ tests/                   # Unit tests
```

## Model Setup

### Required Model Files

The application expects the following files in the `models/` directory:

#### Traditional ML Models
- `svm_model.pkl` - Trained SVM classifier
- `decision_tree_model.pkl` - Trained Decision Tree classifier
- `adaboost_model.pkl` - Trained AdaBoost classifier
- `tfidf_vectorizer.pkl` - Fitted TF-IDF vectorizer

#### Deep Learning Models
- `tokenizer.pkl` - Keras tokenizer for text preprocessing
- `CNN.pkl` - CNN model metadata (contains path to .h5 file)
- `LSTM.pkl` - LSTM model metadata
- `RNN.pkl` - RNN model metadata
- `cnn_model.h5` - CNN model weights
- `lstm_model.h5` - LSTM model weights
- `rnn_model.h5` - RNN model weights

### Model Metadata Format

Deep learning model metadata files (.pkl) should contain:
```python
{
    'model_path': 'path/to/model.h5',
    'max_length': 100,  # Maximum sequence length
    'vocab_size': 10000,  # Vocabulary size
    'embedding_dim': 128,  # Embedding dimension
    'model_type': 'sequential'  # Model architecture type
}
```

##  Configuration

### Environment Variables

You can set these environment variables for customization:

```bash
export TF_CPP_MIN_LOG_LEVEL=2  # Reduce TensorFlow logging
export STREAMLIT_SERVER_PORT=8501  # Custom port
export STREAMLIT_SERVER_ADDRESS=0.0.0.0  # Custom address
```

### Model Configuration

Edit the `configs` dictionary in `app.py` to modify model settings:

```python
configs = {
    'svm': {
        'type': 'sklearn',
        'path': os.path.join(MODEL_DIR, 'svm_model.pkl'),
        'name': 'Support Vector Machine'
    },
    # ... other models
}
```

##  Running the Application

### 1. Start the Streamlit Server

```bash
streamlit run app.py
```

### 2. Access the Application

Open your browser and navigate to:
- Local: `http://localhost:8501`
- Network: `http://your-ip:8501`

### 3. Using the Interface

#### Home Page
- View loaded models and their status
- Get overview of application features

#### Single Detection
1. Select a model from the dropdown
2. Enter text in the text area
3. Click "Analyze" to get predictions
4. View confidence scores and explanations

#### Batch Processing
1. Upload a `.txt` or `.csv` file
2. Select a model
3. Click "Process" to analyze all texts
4. Download results as CSV

#### Model Comparison
1. Enter text for analysis
2. Click "Compare" to see all model predictions
3. View side-by-side results and probability charts

## ðŸ“Š Input Formats

### Single Text
- Plain text in the text area
- No specific format required
- Supports any length (optimal: 50-500 words)

### Batch Files

#### TXT Format
```
Text sample 1
Text sample 2
Text sample 3
```

#### CSV Format
```csv
text,label
"Sample text 1","human"
"Sample text 2","ai"
```

##  Model Details

### Traditional ML Models

#### Support Vector Machine (SVM)
- **Input**: TF-IDF vectors
- **Features**: Word frequency, n-grams
- **Kernel**: RBF (configurable)
- **Preprocessing**: Text cleaning, stop word removal

#### Decision Tree
- **Input**: TF-IDF vectors
- **Features**: Word importance, document structure
- **Splits**: Information gain based
- **Interpretability**: High (feature importance available)

#### AdaBoost
- **Input**: TF-IDF vectors
- **Base Learner**: Decision stumps
- **Ensemble**: Weighted voting
- **Robustness**: Handles overfitting well

### Deep Learning Models

#### CNN (Convolutional Neural Network)
- **Architecture**: 1D convolutions + pooling
- **Features**: Local text patterns, n-gram detection
- **Layers**: Embedding â†’ Conv1D â†’ MaxPooling â†’ Dense
- **Strengths**: Good for pattern recognition in text

#### LSTM (Long Short-Term Memory)
- **Architecture**: Recurrent with memory cells
- **Features**: Sequential dependencies, context awareness
- **Layers**: Embedding â†’ LSTM â†’ Dense
- **Strengths**: Captures long-term dependencies

#### RNN (Recurrent Neural Network)
- **Architecture**: Basic recurrent cells
- **Features**: Sequential processing
- **Layers**: Embedding â†’ SimpleRNN â†’ Dense
- **Strengths**: Simple sequence modeling

##  Customization

### UI Styling

Modify the `init_css()` function to customize appearance:

```python
def init_css():
    st.markdown("""
    <style>
        .main-header { 
            font-size: 3rem; 
            color: #YOUR_COLOR; 
        }
        .ai-pred { 
            background-color: #YOUR_AI_COLOR; 
        }
        .human-pred { 
            background-color: #YOUR_HUMAN_COLOR; 
        }
    </style>
    """, unsafe_allow_html=True)
```

### Model Parameters

Adjust model-specific parameters in the prediction functions:

```python
# For deep learning models
max_length = 200  # Increase for longer texts
vocab_size = 20000  # Increase for larger vocabulary

# For traditional ML models
tfidf_params = {
    'max_features': 10000,
    'ngram_range': (1, 2),
    'stop_words': 'english'
}
```

## Troubleshooting

### Common Issues

#### 1. Models Not Loading
```
Error: No models available
```
**Solution**: Ensure all model files are in the `models/` directory with correct names.

#### 2. TensorFlow Warnings
```
TensorFlow warnings about CPU instructions
```
**Solution**: Set `TF_CPP_MIN_LOG_LEVEL=2` environment variable.

#### 3. Memory Issues
```
ResourceExhaustedError: Out of memory
```
**Solution**: Reduce batch size or use smaller models.

#### 4. Import Errors
```
ModuleNotFoundError: No module named 'streamlit_option_menu'
```
**Solution**: Install missing dependencies: `pip install streamlit-option-menu`

### Performance Optimization

#### 1. Model Loading
- Use model caching with `@st.cache_resource`
- Load models only once at startup
- Implement lazy loading for unused models

#### 2. Prediction Speed
- Batch predictions for multiple texts
- Use GPU for deep learning models
- Optimize tokenization and preprocessing

#### 3. Memory Usage
- Clear model cache periodically
- Use model compression techniques
- Implement model quantization

##  Monitoring and Logging

### Logging Configuration

The application uses Python's logging module:

```python
import logging

logging.basicConfig(
    format="%(levelname)s:%(message)s",
    level=logging.INFO,
    handlers=[
        logging.FileHandler("app.log"),
        logging.StreamHandler()
    ]
)
```

### Metrics Tracking

Monitor these key metrics:
- Prediction accuracy
- Response time
- Memory usage
- Model load time

## Testing

### Unit Tests

Create tests for key functions:

```python
import unittest
from app import make_prediction_debug, load_all_models_debug

class TestPredictions(unittest.TestCase):
    def setUp(self):
        self.models = load_all_models_debug()
    
    def test_prediction_format(self):
        result = make_prediction_debug("Test text", "svm", self.models)
        self.assertIsInstance(result, tuple)
        self.assertEqual(len(result), 3)
```

### Integration Tests

Test the complete workflow:
1. Model loading
2. Text preprocessing
3. Prediction generation
4. UI rendering

##  Security Considerations

### Input Validation
- Sanitize user input
- Limit text length
- Validate file uploads

### Model Security
- Verify model integrity
- Use secure model storage
- Implement access controls

### Data Privacy
- Don't log sensitive text
- Implement data retention policies
- Use secure connections

## Performance Benchmarks

### Model Comparison

| Model | Accuracy | Speed (ms) | Memory (MB) |
|-------|----------|------------|-------------|
| SVM | 85.2% | 50 | 200 |
| Decision Tree | 82.1% | 25 | 150 |
| AdaBoost | 86.3% | 75 | 300 |
| CNN | 88.7% | 200 | 500 |
| LSTM | 90.1% | 300 | 600 |
| RNN | 87.4% | 250 | 450 |

### Hardware Requirements

- **Minimum**: 4GB RAM, 2 CPU cores
- **Recommended**: 8GB RAM, 4 CPU cores
- **Optimal**: 16GB RAM, 8 CPU cores, GPU




